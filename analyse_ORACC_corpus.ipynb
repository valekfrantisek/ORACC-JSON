{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse ORACC corpus\n",
    "\n",
    "Functions provided in this notebook serve to create a referential database that may then be used to execute intertextual search. It partially reuses the script that has been created by Niek Veldhuis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from gensim import corpora\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "import editdistance\n",
    "# from Levenshtein import distance\n",
    "# NOTE: editdistance is slightly faster than Levenshtein --> applied here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "PROJECTS_DATA_PATH = os.path.join(ROOT_PATH, 'projectsdata')\n",
    "CORPUS_PATH = os.path.join(ROOT_PATH, 'CORPUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO list:\n",
    "\n",
    "- create functions that extract data from JSON files\n",
    "- create functions that provide fata suitable for different types of intertextuality detection (all with specified length of match by \"word\"):\n",
    "    - precise intertextuality in cuneiform\n",
    "    - precise intertextuality in normalised form\n",
    "    - intertextuality by lemma\n",
    "    - intertextuality by lemma with Levenshtein distance\n",
    "    - intertextuality by lemma used within a text\n",
    "\n",
    "- It will be needed to create a vectorized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text, parameters, text_id=None):\n",
    "    text_forms = []\n",
    "    text_lemma = []\n",
    "    text_normalised = []\n",
    "    text_signs = []\n",
    "    lemm_l = []\n",
    "    \n",
    "    try:\n",
    "        JSONobject = text['cdl'][0]\n",
    "    except KeyError:\n",
    "        print(f'\\t\\t{text_id} >>> no text data found')\n",
    "        return {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'lemm_l': []}\n",
    "    \n",
    "    try:\n",
    "        if JSONobject['node'] == 'd':\n",
    "            print(f'\\t\\t{text_id} >>> no text data found')\n",
    "            return {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [],'lemm_l': []}\n",
    "        \n",
    "    except KeyError:\n",
    "        print(JSONobject, 'missing node')\n",
    "        \n",
    "    i = 0\n",
    "    while i <= 10:\n",
    "        if type(JSONobject) == list:\n",
    "            for json_dict in JSONobject:\n",
    "                if 'cdl' in json_dict:\n",
    "                    JSONobject = json_dict['cdl']\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        elif type(JSONobject) == dict:\n",
    "            if 'cdl' in JSONobject:\n",
    "                JSONobject = JSONobject['cdl']\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    for inner_json in JSONobject:\n",
    "        if 'label' in inner_json:\n",
    "            parameters['label'] = inner_json['label']\n",
    "        if 'f' in inner_json:\n",
    "            lemma = inner_json['f']\n",
    "            lemma['id_word'] = inner_json['ref']\n",
    "            lemma['label'] = parameters['label']\n",
    "            lemma['id_text'] = parameters['id_text']\n",
    "            \n",
    "            try:\n",
    "                text_forms.append(lemma['form'])\n",
    "            except KeyError:\n",
    "                text_forms.append('UNKNOWN')\n",
    "                lemma['form'] = 'UNKNOWN'\n",
    "            try:\n",
    "                text_lemma.append(lemma['cf'])\n",
    "            except KeyError:\n",
    "                text_lemma.append('UNKNOWN')\n",
    "                lemma['cf'] = 'UNKNOWN'\n",
    "            try:\n",
    "                text_normalised.append(lemma['norm'])\n",
    "            except KeyError:\n",
    "                text_normalised.append('UNKNOWN')\n",
    "                lemma['norm'] = 'UNKNOWN'\n",
    "                \n",
    "            try:\n",
    "                for sign in lemma['gdl']:\n",
    "                    try:\n",
    "                        s = sign['v']\n",
    "                        text_signs.append(s)\n",
    "                    except KeyError:\n",
    "                        text_signs.append('UNKNOWN')\n",
    "                        \n",
    "            except KeyError:\n",
    "                text_signs.append('UNKNOWN')                    \n",
    "                \n",
    "            lemm_l.append(lemma)\n",
    "            \n",
    "        if 'strict' in inner_json and inner_json['strict'] == '1':\n",
    "            lemma = {key: inner_json[key] for key in parameters['dollar_keys']}\n",
    "            lemma['id_word'] = inner_json['ref'] + '.0'\n",
    "            lemma['id_text'] = parameters['id_text']\n",
    "            #lemm_l.append(lemma)\n",
    "    \n",
    "    return {'text_forms': text_forms, 'text_lemma': text_lemma, 'text_normalised': text_normalised, 'text_signs': text_signs, 'lemm_l': lemm_l}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corpusjson_folders(start_path):\n",
    "    corpusjson_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        if \"corpusjson\" in dirs:\n",
    "            relative_path = os.path.relpath(os.path.join(root, \"corpusjson\"), start_path)\n",
    "            corpusjson_paths.append(relative_path.replace(\"\\\\\", \"/\"))\n",
    "            dirs.remove(\"corpusjson\")  # Prevent recursion to the corpusjson folder\n",
    "    \n",
    "    return corpusjson_paths\n",
    "\n",
    "\n",
    "def extract_jsons_from_project(project_name:str):\n",
    "    texts_with_errors = []\n",
    "    \n",
    "    # Find corpusjson folders in the project:\n",
    "    corpusjson_folders = find_corpusjson_folders(os.path.join(PROJECTS_DATA_PATH, project_name))\n",
    "                \n",
    "    project_jsons = {}\n",
    "    \n",
    "    for corpusjson_folder in corpusjson_folders:\n",
    "        full_path = os.path.join(PROJECTS_DATA_PATH, project_name, corpusjson_folder)\n",
    "        text_id_prefix = f'{project_name}/{corpusjson_folder[:-11]}'\n",
    "        files_in_folder = os.listdir(full_path)\n",
    "        if len(files_in_folder) > 0:\n",
    "            print(f'Found {len(files_in_folder)} files in {project_name}/{corpusjson_folder[:-11]} project')\n",
    "            \n",
    "            for json_file_name in files_in_folder:\n",
    "                with open(os.path.join(full_path, json_file_name), 'r', encoding='utf-8') as json_file:\n",
    "                    text_id = f'{text_id_prefix}/{json_file_name[:-5]}'.replace('//', '/') # in case there are no subprojects, there are double slashes --> remove them\n",
    "                    #print(text_id)\n",
    "                    try:\n",
    "                        json_data = json.load(json_file)\n",
    "                        project_jsons[text_id] = json_data\n",
    "                    except:\n",
    "                        texts_with_errors.append(text_id)\n",
    "                    \n",
    "    return project_jsons, texts_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "' Transforming corpus for intertextuality queries. '\n",
    "\n",
    "class OraccProjectCorpus:\n",
    "    def __init__(self, json_corpus):\n",
    "        self.corpus = json_corpus\n",
    "        self.texts =  [text_id for text_id in json_corpus]\n",
    "        self.texts_data = [json_corpus[text_id] for text_id in json_corpus]\n",
    "        self.size = len(json_corpus)\n",
    "        \n",
    "        analysed_corpus = self.AnalyseCorpus()\n",
    "        \n",
    "        self.Lemma = analysed_corpus['lemma']\n",
    "        self.Forms = analysed_corpus['forms']\n",
    "        self.Normalised = analysed_corpus['normalised']\n",
    "        self.Signs = analysed_corpus['signs']\n",
    "        \n",
    "        self.FullCorpus = analysed_corpus['corpus_data']\n",
    "        \n",
    "    def AnalyseCorpus(self) -> dict: \n",
    "        parameters = {'label': None, 'id_text': None, 'dollar_keys' : ['extent', 'scope', 'state']}\n",
    "        \n",
    "        corpus_data = {}\n",
    "        \n",
    "        full_corpus_forms = []\n",
    "        full_corpus_lemma = []\n",
    "        full_corpus_normalised = []\n",
    "        full_corpus_signs = []\n",
    "        \n",
    "        print('\\tAnalyzing texts in the corpus.', self.size, 'texts to be processed.')\n",
    "        \n",
    "        for text_id in self.texts:\n",
    "            parameters['id_text'] = text_id\n",
    "            \n",
    "            try:\n",
    "                if text_id == 'cams/gkab/P338333':\n",
    "                    print(text_id, 'has not been processed but it then got stuck!')\n",
    "                    text_analysed = {'text_forms': [], 'text_lemma': [], 'text_normalised': [], 'text_signs': [], 'lemm_l': []}\n",
    "                else:\n",
    "                    text_analysed = parsejson(self.corpus[text_id], parameters=parameters, text_id=text_id)\n",
    "            except:\n",
    "                # TODO: find out the problems with these texts!\n",
    "                print('ERROR with a text:', text_id)\n",
    "            \n",
    "            corpus_data[text_id] = text_analysed\n",
    "            \n",
    "            full_corpus_forms.append(text_analysed['text_forms'])\n",
    "            full_corpus_lemma.append(text_analysed['text_lemma'])\n",
    "            full_corpus_normalised.append(text_analysed['text_normalised'])\n",
    "            full_corpus_signs.append(text_analysed['text_signs'])\n",
    "            \n",
    "        return {'corpus_data': corpus_data, 'forms': full_corpus_forms, 'lemma': full_corpus_lemma, 'normalised': full_corpus_normalised, 'signs': full_corpus_signs}\n",
    "    \n",
    "        \n",
    "class OraccCorpus():\n",
    "    def __init__(self, input_projects:dict) -> None:\n",
    "        self.projects = input_projects\n",
    "        \n",
    "        lemma_corpus = []\n",
    "        form_corpus = []\n",
    "        normalised_corpus = []\n",
    "        signs_corpus = []\n",
    "    \n",
    "        for project_name, project_data in input_projects.items():\n",
    "            print(project_name, 'is being processed for dictionary.')\n",
    "            OPC_project = OraccProjectCorpus(json_corpus=project_data)\n",
    "            for text in OPC_project.Lemma:\n",
    "                lemma_corpus.append(text)\n",
    "            \n",
    "            for text in OPC_project.Forms:\n",
    "                form_corpus.append(text)\n",
    "                \n",
    "            for text in OPC_project.Normalised:\n",
    "                normalised_corpus.append(text)\n",
    "                \n",
    "            for text in OPC_project.Signs:\n",
    "                signs_corpus.append(text)\n",
    "\n",
    "        lemma_dictionary = corpora.Dictionary(lemma_corpus)\n",
    "        forms_dictionary = corpora.Dictionary(form_corpus)\n",
    "        normalised_dictionary = corpora.Dictionary(normalised_corpus)\n",
    "        signs_dictionary = corpora.Dictionary(signs_corpus)\n",
    "        \n",
    "        self.LemmaDict = lemma_dictionary\n",
    "        self.FormsDict = forms_dictionary\n",
    "        self.NormalisedDict = normalised_dictionary\n",
    "        self.SignsDict = signs_dictionary\n",
    "        \n",
    "        vectors = self.VectorizeOracc()\n",
    "        \n",
    "        self.VectLemma = vectors['vect_lemma']\n",
    "        self.VectForms = vectors['vect_forms']\n",
    "        self.VectNormalised = vectors['vect_norm']\n",
    "        self.VectSigns = vectors['vect_signs']\n",
    "        \n",
    "        self.VectLemmaStream = vectors['vect_lemma_stream']\n",
    "        self.VectFormsStream = vectors['vect_forms_stream']\n",
    "        self.VectNormalisedStream = vectors['vect_norm_stream']\n",
    "        self.VectSignsStream = vectors['vect_signs_stream']\n",
    "        \n",
    "        self.TextsAssociatedToLemma = vectors['lemma_to_texts']\n",
    "        self.TextsAssociatedToForms = vectors['forms_to_texts']\n",
    "        self.TextsAssociatedToNormalised = vectors['norms_to_texts']\n",
    "        self.TextsAssociatedToSigns = vectors['signs_to_texts']\n",
    "        \n",
    "        \n",
    "    def VectorizeOracc(self) -> dict:\n",
    "        vectorized_texts_lemma = {}\n",
    "        vectorized_texts_forms = {}\n",
    "        vectorized_texts_normalised = {}\n",
    "        vectorized_texts_signs = {}\n",
    "        \n",
    "        vectorized_texts_lemma_stream = {}\n",
    "        vectorized_texts_forms_stream = {}\n",
    "        vectorized_texts_normalised_stream = {}\n",
    "        vectorized_texts_signs_stream = {}\n",
    "        \n",
    "        for project_name, project_data in self.projects.items():\n",
    "            print(project_name, 'is being vectorized.')\n",
    "            OPC_project = OraccProjectCorpus(json_corpus=project_data)\n",
    "            analysed_project = OPC_project.FullCorpus\n",
    "            \n",
    "            for text_id, text_data in analysed_project.items():\n",
    "                if text_id not in vectorized_texts_lemma:\n",
    "                    vectorized_texts_lemma[text_id] = self.LemmaDict.doc2bow(text_data['text_lemma'])\n",
    "                    vectorized_texts_lemma_stream[text_id] = self.LemmaDict.doc2idx(text_data['text_lemma'])\n",
    "                else:\n",
    "                    print('ERROR', text_id, 'is duplicit')\n",
    "                \n",
    "                if text_id not in vectorized_texts_forms:\n",
    "                    vectorized_texts_forms[text_id] = self.FormsDict.doc2bow(text_data['text_forms'])\n",
    "                    vectorized_texts_forms_stream[text_id] = self.FormsDict.doc2idx(text_data['text_forms'])\n",
    "                else:\n",
    "                    print('ERROR', text_id, 'is duplicit')\n",
    "                    \n",
    "                if text_id not in vectorized_texts_normalised:\n",
    "                    vectorized_texts_normalised[text_id] = self.NormalisedDict.doc2bow(text_data['text_normalised'])\n",
    "                    vectorized_texts_normalised_stream[text_id] = self.NormalisedDict.doc2idx(text_data['text_normalised'])\n",
    "                else:\n",
    "                    print('ERROR', text_id, 'is duplicit')\n",
    "                    \n",
    "                if text_id not in vectorized_texts_signs:\n",
    "                    vectorized_texts_signs[text_id] = self.SignsDict.doc2bow(text_data['text_signs'])\n",
    "                    vectorized_texts_signs_stream[text_id] = self.SignsDict.doc2idx(text_data['text_signs'])\n",
    "                    \n",
    "        # NOTE: we also make dictionary of tokens and list in which texts they appear. This helps to narrow down texts for possible intertextualities, and speeds up the process.\n",
    "        lemma_to_texts = defaultdict(list)\n",
    "        forms_to_texts = defaultdict(list)\n",
    "        norms_to_texts = defaultdict(list)\n",
    "        signs_to_texts = defaultdict(list)\n",
    "                \n",
    "        for text_id in vectorized_texts_lemma:\n",
    "            for token in vectorized_texts_lemma[text_id]:\n",
    "                token_id = token[0]\n",
    "                lemma_to_texts[token_id].append(text_id)\n",
    "            \n",
    "            for token in vectorized_texts_forms[text_id]:\n",
    "                token_id = token[0]           \n",
    "                forms_to_texts[token_id].append(text_id)\n",
    "            \n",
    "            for token in vectorized_texts_normalised[text_id]:\n",
    "                token_id = token[0]\n",
    "                norms_to_texts[token_id].append(text_id)\n",
    "                \n",
    "            for token in vectorized_texts_signs[text_id]:\n",
    "                token_id = token[0]\n",
    "                signs_to_texts[token_id].append(text_id)\n",
    "\n",
    "        return {'vect_lemma': vectorized_texts_lemma, \n",
    "                'vect_forms': vectorized_texts_forms, \n",
    "                'vect_norm': vectorized_texts_normalised,\n",
    "                'vect_signs': vectorized_texts_signs,\n",
    "                'vect_lemma_stream': vectorized_texts_lemma_stream,\n",
    "                'vect_forms_stream': vectorized_texts_forms_stream,\n",
    "                'vect_norm_stream': vectorized_texts_normalised_stream,\n",
    "                'vect_signs_stream': vectorized_texts_signs_stream,\n",
    "                'lemma_to_texts': lemma_to_texts,\n",
    "                'forms_to_texts': forms_to_texts,\n",
    "                'norms_to_texts': norms_to_texts,\n",
    "                'signs_to_texts': signs_to_texts}\n",
    "        \n",
    "    \n",
    "    def save_full(self, save_name:str, save_path=CORPUS_PATH):\n",
    "        joblib.dump(self, os.path.join(save_path, f'{save_name}.joblib'))\n",
    "    \n",
    "    \n",
    "    def save_corpus(self, corpus_name:str, save_path=CORPUS_PATH):\n",
    "        corpus = {'lemma': self.VectLemma,\n",
    "                  'forms': self.VectForms,\n",
    "                  'norms': self.VectNormalised,\n",
    "                  'signs': self.VectSigns,\n",
    "                  'lemmaStream': self.VectLemmaStream,\n",
    "                  'formStream': self.VectFormsStream,\n",
    "                  'normsStream': self.VectNormalisedStream,\n",
    "                  'signsStream': self.VectSignsStream,}\n",
    "        \n",
    "        joblib.dump(corpus, os.path.join(save_path, f'{corpus_name}.joblib'))\n",
    "        \n",
    "        \n",
    "    def save_dictionaries(self, dictionary_name:str, save_path=CORPUS_PATH):\n",
    "        dictionaries = {'lemma': self.LemmaDict,\n",
    "                        'forms': self.FormsDict,\n",
    "                        'norms': self.NormalisedDict,\n",
    "                        'signs': self.SignsDict}\n",
    "        \n",
    "        joblib.dump(dictionaries, os.path.join(save_path, f'{dictionary_name}.joblib'))\n",
    "        \n",
    "        \n",
    "def load_corpus(corpus_name:str, load_path=CORPUS_PATH):\n",
    "    return joblib.load(os.path.join(load_path, f'{corpus_name}.joblib'))\n",
    "\n",
    "\n",
    "def load_dictionary(dictionary_name:str, load_path=CORPUS_PATH):\n",
    "    return joblib.load(os.path.join(load_path, f'{dictionary_name}.joblib'))\n",
    "\n",
    "\n",
    "def load_OraccCorpus(OraccCorpus_name:str, load_path=CORPUS_PATH) -> OraccCorpus:\n",
    "    return joblib.load(os.path.join(load_path, f'{OraccCorpus_name}.joblib'))\n",
    "\n",
    "def save_json_corpus(json_corpus:dict, save_name:str, save_path=CORPUS_PATH):\n",
    "    joblib.dump(json_corpus, os.path.join(save_path, f'{save_name}.joblib'))\n",
    "    \n",
    "def load_json_corpus(json_corpus_name:str, load_path=CORPUS_PATH) -> dict:\n",
    "    return joblib.load(os.path.join(load_path, f'{json_corpus_name}.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Transforming query text for analysis. \"\"\"\n",
    "\n",
    "def change_unknowns(input_list:list):\n",
    "    return ['UKNOWN_QUERY' if x == 'UNKNOWN' or x == 'x' else x for x in input_list]\n",
    "\n",
    "\n",
    "class ORACCQueryDocument():\n",
    "    def __init__(self, input_text_json:dict, text_id:str, dictionary: corpora.Dictionary) -> None:\n",
    "        self.dict = dictionary\n",
    "        self.textID = text_id\n",
    "        self.JSON = input_text_json\n",
    "        \n",
    "        analysed_corpus = self.AnalyseText()\n",
    "        \n",
    "        self.Lemma = analysed_corpus['lemma']\n",
    "        self.Forms = analysed_corpus['forms']\n",
    "        self.Normalised = analysed_corpus['normalised']\n",
    "        self.Signs = analysed_corpus['signs']\n",
    "        \n",
    "        vectors = self.VectorizeQuery()\n",
    "        \n",
    "        self.boLemma = vectors['QboLemma']\n",
    "        self.boForms = vectors['QboForms']\n",
    "        self.boNorms = vectors['QboNorms']\n",
    "        self.boSigns = vectors['QboSigns']\n",
    "        \n",
    "        self.LemmaStream = vectors['QLemmaStream']\n",
    "        self.FormsStream = vectors['QFormsStream']\n",
    "        self.NormsStream = vectors['QNormsStream']\n",
    "        self.SignsStream = vectors['QSignsStream']\n",
    "        \n",
    "    def AnalyseText(self) -> dict:\n",
    "        parameters = {\"label\": None, \"id_text\": None, \"dollar_keys\" : [\"extent\", \"scope\", \"state\"], \"id_text\": self.textID}\n",
    "        \n",
    "        try:\n",
    "            text_analysed = parsejson(self.JSON, parameters=parameters) \n",
    "        except:\n",
    "            # TODO: find out the problems with these texts!\n",
    "            print('ERROR with a text:', self.textID)\n",
    "            \n",
    "        text_forms = change_unknowns(text_analysed['text_forms'])\n",
    "        text_lemma = change_unknowns(text_analysed['text_lemma'])\n",
    "        text_normalised = change_unknowns(text_analysed['text_normalised'])\n",
    "        text_signs = change_unknowns(text_analysed['text_signs'])\n",
    "            \n",
    "        return {'forms': text_forms, 'lemma': text_lemma, 'normalised': text_normalised, 'signs': text_signs}\n",
    "    \n",
    "    def VectorizeQuery(self) -> dict:\n",
    "        query_boLemma = self.dict['lemma'].doc2bow(self.Lemma)\n",
    "        query_boForms = self.dict['forms'].doc2bow(self.Forms)\n",
    "        query_boNorms = self.dict['norms'].doc2bow(self.Normalised)\n",
    "        query_boSigns = self.dict['signs'].doc2bow(self.Signs)\n",
    "        \n",
    "        query_LemmaStream = self.dict['lemma'].doc2idx(self.Lemma)\n",
    "        query_FormsStream = self.dict['forms'].doc2idx(self.Forms)\n",
    "        query_NormsStream = self.dict['norms'].doc2idx(self.Normalised)\n",
    "        query_SignsStream = self.dict['signs'].doc2idx(self.Signs)\n",
    "        \n",
    "        return {'QboLemma': query_boLemma,\n",
    "                'QboForms': query_boForms,\n",
    "                'QboNorms': query_boNorms,\n",
    "                'QboSigns': query_boSigns,\n",
    "                'QLemmaStream': query_LemmaStream,\n",
    "                'QFormsStream': query_FormsStream,\n",
    "                'QNormsStream': query_NormsStream,\n",
    "                'QSignsStream': query_SignsStream}\n",
    "            \n",
    "            \n",
    "class TEXTQueryDocument():\n",
    "    # TODO: prepare this for the processed data as produced by the \"Cuneiform Text Analyser\" and add references.\n",
    "    def __init__(self, input_text:list, mode='forms') -> None:\n",
    "        \"\"\" :param mode: select from 'lemma', 'forms', 'normalised', or 'signs'. By default, this is set to 'forms' as this is probably the most common input to receive. \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def select_texts_for_intertextualities(tokens_in_query, tokens_in_query_corpus, score_threshold=5):\n",
    "    \"\"\" This function selects texts from the corpus that contain the tokens from the query text. \"\"\"\n",
    "    texts_score = defaultdict(int)\n",
    "    \n",
    "    for token_id in tokens_in_query:\n",
    "        for text_id in tokens_in_query_corpus[token_id]:\n",
    "            texts_score[text_id] += 1\n",
    "        \n",
    "    return [text_id for text_id, score in texts_score.items() if score >= score_threshold]\n",
    "\n",
    "\n",
    "def parse_query_text(full_query: list, window_token_len=5):\n",
    "    \"\"\" This function parses the query text into parts that are then used for the search of intertextualities. \"\"\"\n",
    "    if window_token_len > len(full_query):\n",
    "        return [full_query]\n",
    "    return [full_query[i:i+window_token_len] for i in range(len(full_query) - window_token_len + 1)]\n",
    "\n",
    "\n",
    "def transform_vector_to_text(vector, dictionary: corpora.Dictionary):\n",
    "    output_text = ''\n",
    "    for token_id in vector:\n",
    "        if token_id not in dictionary:\n",
    "            output_text += 'UNKNOWN '\n",
    "        else:\n",
    "            output_text += dictionary[token_id] + ' '\n",
    "            \n",
    "    return output_text.strip()\n",
    "\n",
    "\n",
    "def process_query_text_ORACC(query_document:ORACCQueryDocument, query_corpus:OraccCorpus, mode='lemma', window_token_len=5, tolerance=0, ignore_self=True):\n",
    "    \"\"\"\n",
    "    ADD FUNCTION DESCRIPTION + PARAMS\n",
    "    :param mode: select from 'lemma', 'forms', 'normalised', 'signs'\n",
    "    :param window_token_len: number of tokens to seek for intertextualities; if 0, then the general proximity of full query text is considered (comparison of tokens throughout corpus and query text, not considering token order)\n",
    "    :param tolerance: number of tokens that may differ in the given window_token_len, but still be considered a match.\n",
    "    \"\"\"\n",
    "    \n",
    "    texts_with_intertextualities = []\n",
    "    detailed_intertextualities = defaultdict(list)\n",
    "    \n",
    "    if window_token_len == 0:\n",
    "        # TODO: create function that provides complete general comparison of the query text with the corpus, according to the mode.\n",
    "        return None\n",
    "    \n",
    "    if mode == 'lemma':\n",
    "        vectorised_query = query_document.LemmaStream\n",
    "        vectorised_query_corpus = query_corpus.VectLemmaStream\n",
    "        tokens_in_query_corpus = query_corpus.TextsAssociatedToLemma\n",
    "        dictionary = query_corpus.LemmaDict\n",
    "    elif mode == 'forms':\n",
    "        vectorised_query = query_document.FormsStream\n",
    "        vectorised_query_corpus = query_corpus.VectFormsStream\n",
    "        tokens_in_query_corpus = query_corpus.TextsAssociatedToForms\n",
    "        dictionary = query_corpus.FormsDict\n",
    "    elif mode == 'normalised':\n",
    "        vectorised_query = query_document.NormsStream\n",
    "        vectorised_query_corpus = query_corpus.VectNormalisedStream\n",
    "        tokens_in_query_corpus = query_corpus.TextsAssociatedToNormalised\n",
    "        dictionary = query_corpus.NormalisedDict\n",
    "    elif mode == 'signs':\n",
    "        vectorised_query = query_document.SignsStream\n",
    "        vectorised_query_corpus = query_corpus.VectSignsStream\n",
    "        tokens_in_query_corpus = query_corpus.TextsAssociatedToSigns\n",
    "        dictionary = query_corpus.SignsDict\n",
    "        \n",
    "    parsed_queries = parse_query_text(vectorised_query, window_token_len=window_token_len)    \n",
    "    for query in parsed_queries:\n",
    "        query_transformed = transform_vector_to_text(query, dictionary)\n",
    "        tokens_in_query = list(set(query))\n",
    "        texts_for_intertextualities = select_texts_for_intertextualities(tokens_in_query, tokens_in_query_corpus, score_threshold=window_token_len - tolerance)\n",
    "        # print(query, texts_for_intertextualities)\n",
    "        \n",
    "        for text_id in texts_for_intertextualities:\n",
    "            if ignore_self and text_id == query_document.textID:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                text_vector = vectorised_query_corpus[text_id]\n",
    "                parsed_text = parse_query_text(text_vector, window_token_len=window_token_len)\n",
    "                parse_position_start = -1\n",
    "                for text_to_query_in in parsed_text:\n",
    "                    parse_position_start += 1\n",
    "                    if editdistance.eval(query, text_to_query_in) <= tolerance:\n",
    "                        intertext_found = transform_vector_to_text(text_to_query_in, dictionary)\n",
    "                        texts_with_intertextualities.append(text_id)\n",
    "                        detailed_intertextualities[text_id].append({'query': query_transformed, 'detected_intertextuality': intertext_found, 'position': parse_position_start})\n",
    "                        \n",
    "                        print('Intertextuality found in text:', text_id, 'QUERY:', query_transformed, 'DISCOVERED PART:', intertext_found)\n",
    "                        \n",
    "    return list(set(texts_with_intertextualities)), detailed_intertextualities\n",
    "\n",
    "def process_query_text_TEXT(query_document:TEXTQueryDocument, query_corpus:OraccCorpus, mode='lemma', window_token_len=5, tolerance=0):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 files in adsd/adart1 project\n",
      "Found 162 files in adsd/adart2 project\n",
      "Found 156 files in adsd/adart3 project\n",
      "Found 105 files in adsd/adart5 project\n",
      "Found 180 files in adsd/adart6 project\n",
      "Found 1 files in aemw/alalakh/idrimi project\n",
      "Found 305 files in aemw/amarna project\n",
      "Found 32 files in akklove/ project\n",
      "Found 175 files in ario/ project\n",
      "Found 160 files in asbp/ninmed project\n",
      "Found 44 files in atae/burmarina project\n",
      "Found 408 files in atae/durkatlimmu project\n",
      "Found 212 files in atae/guzana project\n",
      "Found 19 files in atae/huzirina project\n",
      "Found 30 files in atae/imgurenlil project\n",
      "Found 74 files in atae/mallanate project\n",
      "Found 46 files in atae/marqasu project\n",
      "Found 2 files in atae/samal project\n",
      "Found 22 files in atae/szibaniba project\n",
      "Found 22 files in atae/tilbarsip project\n",
      "Found 33 files in atae/tuszhan project\n",
      "Found 225 files in babcity/ project\n",
      "Found 229 files in blms/ project\n",
      "Found 224 files in borsippa/ project\n",
      "Found 132 files in btto/ project\n",
      "Found 3 files in cams/barutu project\n",
      "Found 585 files in cams/gkab project\n",
      "Found 59 files in cams/ludlul project\n",
      "Found 10 files in cams/tlab project\n",
      "Found 92 files in ckst/ project\n",
      "Found 49 files in cmawro/cmawr1 project\n",
      "Found 82 files in cmawro/cmawr2 project\n",
      "Found 124 files in cmawro/cmawr3 project\n",
      "Found 9 files in cmawro/maqlu project\n",
      "Found 378 files in contrib/amarna project\n",
      "Found 4953 files in dcclt/ project\n",
      "Found 156 files in dcclt/ebla project\n",
      "Found 670 files in dcclt/nineveh project\n",
      "Found 160 files in dcclt/signlists project\n",
      "Found 252 files in dccmt/ project\n",
      "Found 535 files in dsst/ project\n",
      "Found 816 files in ecut/ project\n",
      "Found 976 files in edlex/ project\n",
      "Found 523 files in eisl/ project\n",
      "Found 1456 files in etcsri/ project\n",
      "Found 20 files in glass/ project\n",
      "Found 487 files in hbtin/ project\n",
      "Found 9 files in iraq/iraq85 project\n",
      "Found 1 files in nere/ project\n",
      "Found 411 files in obel/ project\n",
      "Found 201 files in obmc/ project\n",
      "Found 35 files in obta/ project\n",
      "Found 40 files in ribo/bab7scores project\n",
      "Found 1 files in ribo/babylon10 project\n",
      "Found 38 files in ribo/babylon2 project\n",
      "Found 4 files in ribo/babylon3 project\n",
      "Found 6 files in ribo/babylon4 project\n",
      "Found 1 files in ribo/babylon5 project\n",
      "Found 126 files in ribo/babylon6 project\n",
      "Found 232 files in ribo/babylon7 project\n",
      "Found 3 files in ribo/babylon8 project\n",
      "Found 400 files in ribo/sources project\n",
      "Found 378 files in rimanum/ project\n",
      "Found 96 files in rinap/rinap1 project\n",
      "Found 151 files in rinap/rinap2 project\n",
      "Found 261 files in rinap/rinap3 project\n",
      "Found 183 files in rinap/rinap4 project\n",
      "Found 346 files in rinap/rinap5 project\n",
      "Found 129 files in rinap/scores project\n",
      "Found 2209 files in rinap/sources project\n",
      "Found 265 files in saao/saa01 project\n",
      "Found 15 files in saao/saa02 project\n",
      "Found 52 files in saao/saa03 project\n",
      "Found 353 files in saao/saa04 project\n",
      "Found 300 files in saao/saa05 project\n",
      "Found 350 files in saao/saa06 project\n",
      "Found 219 files in saao/saa07 project\n",
      "Found 568 files in saao/saa08 project\n",
      "Found 11 files in saao/saa09 project\n",
      "Found 389 files in saao/saa10 project\n",
      "Found 234 files in saao/saa11 project\n",
      "Found 98 files in saao/saa12 project\n",
      "Found 210 files in saao/saa13 project\n",
      "Found 479 files in saao/saa14 project\n",
      "Found 390 files in saao/saa15 project\n",
      "Found 246 files in saao/saa16 project\n",
      "Found 207 files in saao/saa17 project\n",
      "Found 205 files in saao/saa18 project\n",
      "Found 229 files in saao/saa19 project\n",
      "Found 55 files in saao/saa20 project\n",
      "Found 161 files in saao/saa21 project\n",
      "Found 23 files in saao/saas2 project\n",
      "Found 33 files in suhu/ project\n",
      "Found 147 files in urap/ project\n"
     ]
    }
   ],
   "source": [
    "all_project_jsons = {}\n",
    "projects_texts_with_errors = {}\n",
    "\n",
    "for project_name in os.listdir(PROJECTS_DATA_PATH):\n",
    "    project_jsons, texts_with_errors = extract_jsons_from_project(project_name)\n",
    "    all_project_jsons[project_name] = project_jsons\n",
    "    projects_texts_with_errors[project_name] = texts_with_errors\n",
    "    \n",
    "save_json_corpus(all_project_jsons, 'all_project_jsons')\n",
    "save_json_corpus(projects_texts_with_errors, 'all_projects_texts_with_errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_project_jsons = load_json_corpus('all_project_jsons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsd is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 692 texts to be processed.\n",
      "aemw is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 306 texts to be processed.\n",
      "akklove is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 30 texts to be processed.\n",
      "amgg is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "ario is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 173 texts to be processed.\n",
      "armep is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "arrim is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "asbp is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 159 texts to be processed.\n",
      "atae is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 912 texts to be processed.\n",
      "babcity is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 224 texts to be processed.\n",
      "blms is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 213 texts to be processed.\n",
      "borsippa is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 224 texts to be processed.\n",
      "btmao is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "btto is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 132 texts to be processed.\n",
      "cams is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 656 texts to be processed.\n",
      "\t\tcams/gkab/P296532 >>> no text data found\n",
      "\t\tcams/gkab/P338319 >>> no text data found\n",
      "\t\tcams/gkab/P338320 >>> no text data found\n",
      "\t\tcams/gkab/P338326 >>> no text data found\n",
      "cams/gkab/P338333 has not been processed but it then got stuck!\n",
      "\t\tcams/gkab/P338338 >>> no text data found\n",
      "\t\tcams/gkab/P338350 >>> no text data found\n",
      "\t\tcams/gkab/P338355 >>> no text data found\n",
      "\t\tcams/gkab/P338357 >>> no text data found\n",
      "\t\tcams/gkab/P338374 >>> no text data found\n",
      "\t\tcams/gkab/P338382 >>> no text data found\n",
      "\t\tcams/gkab/P338387 >>> no text data found\n",
      "\t\tcams/gkab/P338388 >>> no text data found\n",
      "\t\tcams/gkab/P338390 >>> no text data found\n",
      "\t\tcams/gkab/P338402 >>> no text data found\n",
      "\t\tcams/gkab/P338404 >>> no text data found\n",
      "\t\tcams/gkab/P338491 >>> no text data found\n",
      "\t\tcams/gkab/P338498 >>> no text data found\n",
      "\t\tcams/gkab/P338511 >>> no text data found\n",
      "\t\tcams/gkab/P338517 >>> no text data found\n",
      "\t\tcams/gkab/P338551 >>> no text data found\n",
      "\t\tcams/gkab/P338556 >>> no text data found\n",
      "\t\tcams/gkab/P338558 >>> no text data found\n",
      "\t\tcams/gkab/P338561 >>> no text data found\n",
      "\t\tcams/gkab/P338566 >>> no text data found\n",
      "\t\tcams/gkab/P338615 >>> no text data found\n",
      "\t\tcams/gkab/P338616 >>> no text data found\n",
      "\t\tcams/gkab/P338620 >>> no text data found\n",
      "\t\tcams/gkab/P338638 >>> no text data found\n",
      "\t\tcams/gkab/P338655 >>> no text data found\n",
      "\t\tcams/gkab/P338657 >>> no text data found\n",
      "\t\tcams/gkab/P338658 >>> no text data found\n",
      "\t\tcams/gkab/P338704 >>> no text data found\n",
      "\t\tcams/gkab/P348423 >>> no text data found\n",
      "\t\tcams/gkab/P348435 >>> no text data found\n",
      "\t\tcams/gkab/P348449 >>> no text data found\n",
      "\t\tcams/gkab/P348450 >>> no text data found\n",
      "\t\tcams/gkab/P348452 >>> no text data found\n",
      "\t\tcams/gkab/P348453 >>> no text data found\n",
      "\t\tcams/gkab/P348464 >>> no text data found\n",
      "\t\tcams/gkab/P348465 >>> no text data found\n",
      "\t\tcams/gkab/P348466 >>> no text data found\n",
      "\t\tcams/gkab/P348470 >>> no text data found\n",
      "\t\tcams/gkab/P348472 >>> no text data found\n",
      "\t\tcams/gkab/P348477 >>> no text data found\n",
      "\t\tcams/gkab/P348480 >>> no text data found\n",
      "\t\tcams/gkab/P348481 >>> no text data found\n",
      "\t\tcams/gkab/P348490 >>> no text data found\n",
      "\t\tcams/gkab/P348492 >>> no text data found\n",
      "\t\tcams/gkab/P348493 >>> no text data found\n",
      "\t\tcams/gkab/P348515 >>> no text data found\n",
      "\t\tcams/gkab/P348517 >>> no text data found\n",
      "\t\tcams/gkab/P348558 >>> no text data found\n",
      "\t\tcams/gkab/P348623 >>> no text data found\n",
      "\t\tcams/gkab/P348630 >>> no text data found\n",
      "\t\tcams/gkab/P348637 >>> no text data found\n",
      "\t\tcams/gkab/P348638 >>> no text data found\n",
      "\t\tcams/gkab/P348639 >>> no text data found\n",
      "\t\tcams/gkab/P348640 >>> no text data found\n",
      "\t\tcams/gkab/P348642 >>> no text data found\n",
      "\t\tcams/gkab/P348643 >>> no text data found\n",
      "\t\tcams/gkab/P348644 >>> no text data found\n",
      "\t\tcams/gkab/P348648 >>> no text data found\n",
      "\t\tcams/gkab/P348649 >>> no text data found\n",
      "\t\tcams/gkab/P348651 >>> no text data found\n",
      "\t\tcams/gkab/P348668 >>> no text data found\n",
      "\t\tcams/gkab/P348684 >>> no text data found\n",
      "\t\tcams/gkab/P348688 >>> no text data found\n",
      "\t\tcams/gkab/P348694 >>> no text data found\n",
      "\t\tcams/gkab/P348698 >>> no text data found\n",
      "\t\tcams/gkab/P348701 >>> no text data found\n",
      "\t\tcams/gkab/P348702 >>> no text data found\n",
      "\t\tcams/gkab/P348703 >>> no text data found\n",
      "\t\tcams/gkab/P348704 >>> no text data found\n",
      "\t\tcams/gkab/P348708 >>> no text data found\n",
      "\t\tcams/gkab/P348709 >>> no text data found\n",
      "\t\tcams/gkab/P348731 >>> no text data found\n",
      "\t\tcams/gkab/P348735 >>> no text data found\n",
      "\t\tcams/gkab/P348743 >>> no text data found\n",
      "\t\tcams/gkab/P348744 >>> no text data found\n",
      "\t\tcams/gkab/P348745 >>> no text data found\n",
      "\t\tcams/gkab/P348754 >>> no text data found\n",
      "\t\tcams/gkab/P348755 >>> no text data found\n",
      "\t\tcams/gkab/P348765 >>> no text data found\n",
      "\t\tcams/gkab/P348766 >>> no text data found\n",
      "\t\tcams/gkab/P348767 >>> no text data found\n",
      "\t\tcams/gkab/P348818 >>> no text data found\n",
      "\t\tcams/gkab/P348828 >>> no text data found\n",
      "\t\tcams/gkab/P348835 >>> no text data found\n",
      "\t\tcams/gkab/P348847 >>> no text data found\n",
      "\t\tcams/gkab/P348849 >>> no text data found\n",
      "\t\tcams/gkab/P348852 >>> no text data found\n",
      "\t\tcams/gkab/P363675 >>> no text data found\n",
      "\t\tcams/gkab/P363682 >>> no text data found\n",
      "\t\tcams/gkab/P363683 >>> no text data found\n",
      "\t\tcams/gkab/P363684 >>> no text data found\n",
      "\t\tcams/gkab/P363688 >>> no text data found\n",
      "\t\tcams/gkab/P363689 >>> no text data found\n",
      "\t\tcams/gkab/P363690 >>> no text data found\n",
      "\t\tcams/gkab/P363692 >>> no text data found\n",
      "\t\tcams/gkab/P363693 >>> no text data found\n",
      "\t\tcams/gkab/P363699 >>> no text data found\n",
      "\t\tcams/gkab/P363710 >>> no text data found\n",
      "\t\tcams/gkab/P363711 >>> no text data found\n",
      "\t\tcams/gkab/P363716 >>> no text data found\n",
      "\t\tcams/gkab/P363717 >>> no text data found\n",
      "\t\tcams/gkab/P363718 >>> no text data found\n",
      "\t\tcams/gkab/P363719 >>> no text data found\n",
      "\t\tcams/gkab/P387619 >>> no text data found\n",
      "\t\tcams/gkab/P392625 >>> no text data found\n",
      "ckst is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 91 texts to be processed.\n",
      "cmawro is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 261 texts to be processed.\n",
      "contrib is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 345 texts to be processed.\n",
      "dcclt is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 5878 texts to be processed.\n",
      "dccmt is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 250 texts to be processed.\n",
      "dsst is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 535 texts to be processed.\n",
      "ecut is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 806 texts to be processed.\n",
      "edlex is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 966 texts to be processed.\n",
      "eisl is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 502 texts to be processed.\n",
      "etcsri is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 1456 texts to be processed.\n",
      "glass is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 19 texts to be processed.\n",
      "hbtin is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 485 texts to be processed.\n",
      "ERROR with a text: hbtin/P296745\n",
      "ERROR with a text: hbtin/P296747\n",
      "ERROR with a text: hbtin/P296750\n",
      "ERROR with a text: hbtin/P296754\n",
      "ERROR with a text: hbtin/P296758\n",
      "ERROR with a text: hbtin/P296770\n",
      "ERROR with a text: hbtin/P303975\n",
      "ERROR with a text: hbtin/P303977\n",
      "ERROR with a text: hbtin/P303978\n",
      "ERROR with a text: hbtin/P303982\n",
      "ERROR with a text: hbtin/P304011\n",
      "ERROR with a text: hbtin/P304337\n",
      "ERROR with a text: hbtin/P304338\n",
      "ERROR with a text: hbtin/P309742\n",
      "iraq is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 9 texts to be processed.\n",
      "nere is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 1 texts to be processed.\n",
      "nimrud is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "obel is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 411 texts to be processed.\n",
      "obmc is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 201 texts to be processed.\n",
      "obta is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 31 texts to be processed.\n",
      "ogsl is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "oimea is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "osl is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "pnao is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "qcat is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "riao is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "ribo is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 851 texts to be processed.\n",
      "rimanum is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 338 texts to be processed.\n",
      "rinap is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 3373 texts to be processed.\n",
      "saao is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 5055 texts to be processed.\n",
      "suhu is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 33 texts to be processed.\n",
      "tcma is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "tsae is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "urap is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 147 texts to be processed.\n",
      "xcat is being processed for dictionary.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "adsd is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 692 texts to be processed.\n",
      "aemw is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 306 texts to be processed.\n",
      "akklove is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 30 texts to be processed.\n",
      "amgg is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "ario is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 173 texts to be processed.\n",
      "armep is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "arrim is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "asbp is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 159 texts to be processed.\n",
      "atae is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 912 texts to be processed.\n",
      "babcity is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 224 texts to be processed.\n",
      "blms is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 213 texts to be processed.\n",
      "borsippa is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 224 texts to be processed.\n",
      "btmao is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "btto is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 132 texts to be processed.\n",
      "cams is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 656 texts to be processed.\n",
      "\t\tcams/gkab/P296532 >>> no text data found\n",
      "\t\tcams/gkab/P338319 >>> no text data found\n",
      "\t\tcams/gkab/P338320 >>> no text data found\n",
      "\t\tcams/gkab/P338326 >>> no text data found\n",
      "cams/gkab/P338333 has not been processed but it then got stuck!\n",
      "\t\tcams/gkab/P338338 >>> no text data found\n",
      "\t\tcams/gkab/P338350 >>> no text data found\n",
      "\t\tcams/gkab/P338355 >>> no text data found\n",
      "\t\tcams/gkab/P338357 >>> no text data found\n",
      "\t\tcams/gkab/P338374 >>> no text data found\n",
      "\t\tcams/gkab/P338382 >>> no text data found\n",
      "\t\tcams/gkab/P338387 >>> no text data found\n",
      "\t\tcams/gkab/P338388 >>> no text data found\n",
      "\t\tcams/gkab/P338390 >>> no text data found\n",
      "\t\tcams/gkab/P338402 >>> no text data found\n",
      "\t\tcams/gkab/P338404 >>> no text data found\n",
      "\t\tcams/gkab/P338491 >>> no text data found\n",
      "\t\tcams/gkab/P338498 >>> no text data found\n",
      "\t\tcams/gkab/P338511 >>> no text data found\n",
      "\t\tcams/gkab/P338517 >>> no text data found\n",
      "\t\tcams/gkab/P338551 >>> no text data found\n",
      "\t\tcams/gkab/P338556 >>> no text data found\n",
      "\t\tcams/gkab/P338558 >>> no text data found\n",
      "\t\tcams/gkab/P338561 >>> no text data found\n",
      "\t\tcams/gkab/P338566 >>> no text data found\n",
      "\t\tcams/gkab/P338615 >>> no text data found\n",
      "\t\tcams/gkab/P338616 >>> no text data found\n",
      "\t\tcams/gkab/P338620 >>> no text data found\n",
      "\t\tcams/gkab/P338638 >>> no text data found\n",
      "\t\tcams/gkab/P338655 >>> no text data found\n",
      "\t\tcams/gkab/P338657 >>> no text data found\n",
      "\t\tcams/gkab/P338658 >>> no text data found\n",
      "\t\tcams/gkab/P338704 >>> no text data found\n",
      "\t\tcams/gkab/P348423 >>> no text data found\n",
      "\t\tcams/gkab/P348435 >>> no text data found\n",
      "\t\tcams/gkab/P348449 >>> no text data found\n",
      "\t\tcams/gkab/P348450 >>> no text data found\n",
      "\t\tcams/gkab/P348452 >>> no text data found\n",
      "\t\tcams/gkab/P348453 >>> no text data found\n",
      "\t\tcams/gkab/P348464 >>> no text data found\n",
      "\t\tcams/gkab/P348465 >>> no text data found\n",
      "\t\tcams/gkab/P348466 >>> no text data found\n",
      "\t\tcams/gkab/P348470 >>> no text data found\n",
      "\t\tcams/gkab/P348472 >>> no text data found\n",
      "\t\tcams/gkab/P348477 >>> no text data found\n",
      "\t\tcams/gkab/P348480 >>> no text data found\n",
      "\t\tcams/gkab/P348481 >>> no text data found\n",
      "\t\tcams/gkab/P348490 >>> no text data found\n",
      "\t\tcams/gkab/P348492 >>> no text data found\n",
      "\t\tcams/gkab/P348493 >>> no text data found\n",
      "\t\tcams/gkab/P348515 >>> no text data found\n",
      "\t\tcams/gkab/P348517 >>> no text data found\n",
      "\t\tcams/gkab/P348558 >>> no text data found\n",
      "\t\tcams/gkab/P348623 >>> no text data found\n",
      "\t\tcams/gkab/P348630 >>> no text data found\n",
      "\t\tcams/gkab/P348637 >>> no text data found\n",
      "\t\tcams/gkab/P348638 >>> no text data found\n",
      "\t\tcams/gkab/P348639 >>> no text data found\n",
      "\t\tcams/gkab/P348640 >>> no text data found\n",
      "\t\tcams/gkab/P348642 >>> no text data found\n",
      "\t\tcams/gkab/P348643 >>> no text data found\n",
      "\t\tcams/gkab/P348644 >>> no text data found\n",
      "\t\tcams/gkab/P348648 >>> no text data found\n",
      "\t\tcams/gkab/P348649 >>> no text data found\n",
      "\t\tcams/gkab/P348651 >>> no text data found\n",
      "\t\tcams/gkab/P348668 >>> no text data found\n",
      "\t\tcams/gkab/P348684 >>> no text data found\n",
      "\t\tcams/gkab/P348688 >>> no text data found\n",
      "\t\tcams/gkab/P348694 >>> no text data found\n",
      "\t\tcams/gkab/P348698 >>> no text data found\n",
      "\t\tcams/gkab/P348701 >>> no text data found\n",
      "\t\tcams/gkab/P348702 >>> no text data found\n",
      "\t\tcams/gkab/P348703 >>> no text data found\n",
      "\t\tcams/gkab/P348704 >>> no text data found\n",
      "\t\tcams/gkab/P348708 >>> no text data found\n",
      "\t\tcams/gkab/P348709 >>> no text data found\n",
      "\t\tcams/gkab/P348731 >>> no text data found\n",
      "\t\tcams/gkab/P348735 >>> no text data found\n",
      "\t\tcams/gkab/P348743 >>> no text data found\n",
      "\t\tcams/gkab/P348744 >>> no text data found\n",
      "\t\tcams/gkab/P348745 >>> no text data found\n",
      "\t\tcams/gkab/P348754 >>> no text data found\n",
      "\t\tcams/gkab/P348755 >>> no text data found\n",
      "\t\tcams/gkab/P348765 >>> no text data found\n",
      "\t\tcams/gkab/P348766 >>> no text data found\n",
      "\t\tcams/gkab/P348767 >>> no text data found\n",
      "\t\tcams/gkab/P348818 >>> no text data found\n",
      "\t\tcams/gkab/P348828 >>> no text data found\n",
      "\t\tcams/gkab/P348835 >>> no text data found\n",
      "\t\tcams/gkab/P348847 >>> no text data found\n",
      "\t\tcams/gkab/P348849 >>> no text data found\n",
      "\t\tcams/gkab/P348852 >>> no text data found\n",
      "\t\tcams/gkab/P363675 >>> no text data found\n",
      "\t\tcams/gkab/P363682 >>> no text data found\n",
      "\t\tcams/gkab/P363683 >>> no text data found\n",
      "\t\tcams/gkab/P363684 >>> no text data found\n",
      "\t\tcams/gkab/P363688 >>> no text data found\n",
      "\t\tcams/gkab/P363689 >>> no text data found\n",
      "\t\tcams/gkab/P363690 >>> no text data found\n",
      "\t\tcams/gkab/P363692 >>> no text data found\n",
      "\t\tcams/gkab/P363693 >>> no text data found\n",
      "\t\tcams/gkab/P363699 >>> no text data found\n",
      "\t\tcams/gkab/P363710 >>> no text data found\n",
      "\t\tcams/gkab/P363711 >>> no text data found\n",
      "\t\tcams/gkab/P363716 >>> no text data found\n",
      "\t\tcams/gkab/P363717 >>> no text data found\n",
      "\t\tcams/gkab/P363718 >>> no text data found\n",
      "\t\tcams/gkab/P363719 >>> no text data found\n",
      "\t\tcams/gkab/P387619 >>> no text data found\n",
      "\t\tcams/gkab/P392625 >>> no text data found\n",
      "ckst is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 91 texts to be processed.\n",
      "cmawro is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 261 texts to be processed.\n",
      "contrib is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 345 texts to be processed.\n",
      "dcclt is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 5878 texts to be processed.\n",
      "dccmt is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 250 texts to be processed.\n",
      "dsst is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 535 texts to be processed.\n",
      "ecut is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 806 texts to be processed.\n",
      "edlex is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 966 texts to be processed.\n",
      "eisl is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 502 texts to be processed.\n",
      "etcsri is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 1456 texts to be processed.\n",
      "glass is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 19 texts to be processed.\n",
      "hbtin is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 485 texts to be processed.\n",
      "ERROR with a text: hbtin/P296745\n",
      "ERROR with a text: hbtin/P296747\n",
      "ERROR with a text: hbtin/P296750\n",
      "ERROR with a text: hbtin/P296754\n",
      "ERROR with a text: hbtin/P296758\n",
      "ERROR with a text: hbtin/P296770\n",
      "ERROR with a text: hbtin/P303975\n",
      "ERROR with a text: hbtin/P303977\n",
      "ERROR with a text: hbtin/P303978\n",
      "ERROR with a text: hbtin/P303982\n",
      "ERROR with a text: hbtin/P304011\n",
      "ERROR with a text: hbtin/P304337\n",
      "ERROR with a text: hbtin/P304338\n",
      "ERROR with a text: hbtin/P309742\n",
      "iraq is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 9 texts to be processed.\n",
      "nere is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 1 texts to be processed.\n",
      "nimrud is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "obel is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 411 texts to be processed.\n",
      "obmc is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 201 texts to be processed.\n",
      "obta is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 31 texts to be processed.\n",
      "ogsl is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "oimea is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "osl is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "pnao is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "qcat is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "riao is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "ribo is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 851 texts to be processed.\n",
      "rimanum is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 338 texts to be processed.\n",
      "rinap is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 3373 texts to be processed.\n",
      "saao is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 5055 texts to be processed.\n",
      "suhu is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 33 texts to be processed.\n",
      "tcma is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "tsae is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n",
      "urap is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 147 texts to be processed.\n",
      "xcat is being vectorized.\n",
      "\tAnalyzing texts in the corpus. 0 texts to be processed.\n"
     ]
    }
   ],
   "source": [
    "CorpusForQueries = OraccCorpus(all_project_jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusForQueries.save_corpus(corpus_name='full_ORACC_corpus')\n",
    "CorpusForQueries.save_dictionaries(dictionary_name='full_ORACC_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_corpus = load_corpus('nere_btto_dsst_corpus_Lemma')\n",
    "loaded_dict = load_dictionary('full_ORACC_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing intertextuality detection\n",
    "\n",
    "TODO:\n",
    "- signs without numbers (en == n == en3)\n",
    "- signs with MZL values (DINGIR == an)\n",
    "- \"normalised signs\" - i.e., im-ma --> imma; si-im --> sim (keep doubble consonants, ignore double vowels)\n",
    "- \"normalised stream\" - do not compare the stream of tokens, but then connect it into string and use distance on the whole stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files in nere/ project\n"
     ]
    }
   ],
   "source": [
    "nere_jsons, nere_errors = extract_jsons_from_project('nere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = ORACCQueryDocument(nere_jsons['nere/Q009326'], 'nere/Q009326', loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_with_intertextualities, detailed_intertextualities = process_query_text_ORACC(test_query, CorpusForQueries, mode='signs', window_token_len=10 , tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(texts_with_intertextualities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
